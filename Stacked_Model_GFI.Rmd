---
title: "Stacked Model (GFI)"
author: "Hip Hip Array"
date: "12/03/2021"
output:
  html_document:
    highlight: tango
    theme: united
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Install packages and Load Data

```{r, cache=TRUE}
library(nflfastR)

future::plan("multisession")
pbp <- load_pbp(2010:2020, file_type = "qs")
str(pbp)

# library(gdata)
# pbp$year <- left(pbp$game_date, n=4)
```

## Set up "Going For It" (GFI) Data
```{r}
# Only pull plays that are a fourth down attempt
GFI <- pbp[!is.na(pbp$play_type), ]
GFI <- GFI[!is.na(GFI$play_id) & GFI$down == 4 & GFI$punt_attempt == 0 & GFI$field_goal_attempt == 0 & GFI$play_type != "no_play" & GFI$play_type != "qb_kneel", ]
```



## Set up GFI data for all models
```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
library(dplyr)

# Include all variables for step function, exclude NA rows
GFI_ALL <- GFI[!is.na(GFI$fourth_down_converted), ]

## Randomize the rows in the data (shuffling the rows)
set.seed(123)
GFI_ALL <- slice(GFI_ALL, sample(1:n()))
```


## Variable Selection with GLM

### Training
```{r, cache=TRUE}
GFI_GLM_ALL <- select(
  GFI_ALL,
  fourth_down_converted,
  season_type,
  week,
  posteam_type,
  yardline_100,
  quarter_seconds_remaining,
  half_seconds_remaining,
  game_seconds_remaining,
  game_half,
  qtr,
  goal_to_go,
  ydstogo,
  play_type,
  shotgun,
  no_huddle,
  home_timeouts_remaining,
  away_timeouts_remaining,
  timeout,
  posteam_timeouts_remaining,
  defteam_timeouts_remaining,
  posteam_score,
  defteam_score,
  score_differential,
  special_teams_play,
  fixed_drive,
  away_score,
  home_score,
  location,
  div_game,
  roof,
  surface,
  home_opening_kickoff
)
GFI_GLM_ALL <- GFI_GLM_ALL %>% mutate_if(is.character, as.factor)

GFI_GLM_ALL <- GFI_GLM_ALL %>% select(where(~ n_distinct(.) > 1))

train_set <- sample(1:nrow(GFI_GLM_ALL), 0.7 * nrow(GFI_GLM_ALL))

# Training set
gfi_glm_train <- GFI_GLM_ALL[train_set, ]
gfi_glm_validation <- GFI_GLM_ALL[-train_set, ]

library(stats)
model_glm <- glm(fourth_down_converted ~ ., data = gfi_glm_train, family = "binomial", control = list(maxit = 100))
model_glm <- step(model_glm, direction = "backward")
summary(model_glm)
```

### Predicting
```{r}
predict_glm <- predict(model_glm, newdata = gfi_glm_validation, type = "response")
predict_glm_factor <- ifelse(predict_glm < 0.5, 0, 1)
predict_glm_factor <- as.factor(predict_glm_factor)
library(caret)
confusionMatrix(predict_glm_factor, as.factor(gfi_glm_validation$fourth_down_converted))
```

Now we will run future models on just inputs the stepwise GLM found significant.

```{r}
# Include only variables we want to use in prediction
GFI_small <- select(
  GFI_ALL,
  fourth_down_converted,
  yardline_100,
  half_seconds_remaining,
  game_half,
  goal_to_go,
  ydstogo,
  play_type,
  shotgun,
  timeout,
  posteam_score,
  defteam_score,
  away_score,
  home_score
)

GFI_small <- GFI_small %>% mutate_if(is.character, as.factor)

GFI_small <- GFI_small %>% select(where(~ n_distinct(.) > 1))

library(janitor)
GFI_small <- clean_names(GFI_small)

# Model Matrix for ANN
GFI_mm <- as.data.frame(model.matrix(~ . - 1, GFI_small))


# Normalize the data
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything
# Create Labels for KNN
GFI_norm <- as.data.frame(lapply(GFI_mm[-1], normalize))
GFI_labels <- GFI_mm[, 1]
```

## GLM

### Training
```{r}
library(stats)
model_glm <- glm(fourth_down_converted ~ ., data = GFI_small, family = "binomial", control = list(maxit = 100))
summary(model_glm)
```

### Predicting
```{r}
predict_glm <- predict(model_glm, newdata = GFI_small, type = "response")
predict_glm_factor <- ifelse(predict_glm < 0.5, 0, 1)
predict_glm_factor <- as.factor(predict_glm_factor)
library(caret)
glmcm <- confusionMatrix(predict_glm_factor, as.factor(GFI_small$fourth_down_converted))
glmcm
```

## ANN

### Training
```{r, cache=TRUE}
library(neuralnet)
model_ann <- neuralnet(fourth_down_converted ~ ., data = GFI_mm, stepmax = 1000000, hidden = 3)
plot(model_ann)
```

### Predicting
```{r}
library(neuralnet)
predict_ann <- compute(model_ann, GFI_mm)
predict_ann <- predict_ann$net.result
predict_ann_factor <- ifelse(predict_ann < 0.5, 0, 1)
predict_ann_factor <- as.factor(predict_ann_factor)

anncm <- confusionMatrix(predict_ann_factor, as.factor(GFI_mm$fourth_down_converted))
anncm
```


## KNN

### Tuning the hyperparameter k
```{r}
library(class)

best_kappa <- 0
best_k <- 0

train_set <- sample(1:nrow(GFI_norm), 0.7 * nrow(GFI_norm))

# Training set
gfi_knn_train <- GFI_norm[train_set, ]
gfi_knn_validation <- GFI_norm[-train_set, ]

gfi_knn_train_labels <- GFI_labels[train_set]
gfi_knn_validation_labels <- GFI_labels[-train_set]

for (i in 1:100) {
  knn_pred <- knn(
    train = gfi_knn_train, test = gfi_knn_validation,
    cl = gfi_knn_train_labels, k = i
  )

  kappa <- confusionMatrix(as.factor(knn_pred), as.factor(gfi_knn_validation_labels))$overall["Kappa"]

  print(i)
  print(kappa)
  if (kappa > best_kappa) {
    best_k <- i
    best_kappa <- kappa
  }
}
```

### Training & Predicting
```{r}
library(class)

knn_pred <- knn(
  train = GFI_norm, test = GFI_norm,
  cl = GFI_labels, k = best_k
)

knncm <- confusionMatrix(as.factor(knn_pred), as.factor(GFI_labels))
knncm
```

## SVM

### Tuning the hyperparemeter sigma

```{r}
library(kernlab)
library(class)

best_kappa <- 0
best_sigma <- 0

train_set <- sample(1:nrow(GFI_small), 0.7 * nrow(GFI_small))

# Training set
gfi_svm_train <- GFI_small[train_set, ]
gfi_svm_validation <- GFI_small[-train_set, ]

for (i in c(2:10 %o% 10^(-5:2))) {
  model_svm <- ksvm(fourth_down_converted ~ ., data = gfi_svm_train, kernel = "rbfdot", kpar = list(sigma = i))
  predict_svm <- predict(model_svm, gfi_svm_validation)
  predict_svm <- ifelse(predict_svm < 0.5, 0, 1)

  kappa <- confusionMatrix(as.factor(predict_svm), as.factor(gfi_svm_validation$fourth_down_converted))$overall["Kappa"]

  print(i)
  print(kappa)
  if (kappa > best_kappa) {
    best_sigma <- i
    best_kappa <- kappa
  }
}
```

### Training
```{r}
library(kernlab)
model_svm <- ksvm(fourth_down_converted ~ ., data = GFI_small, kernel = "rbfdot", kpar = list(sigma = best_sigma))
```

### Predicting
```{r}
predict_svm <- predict(model_svm, GFI_small)
predict_svm <- ifelse(predict_svm < 0.5, 0, 1)

svmcm <- confusionMatrix(as.factor(predict_svm), as.factor(GFI_labels))
svmcm
```

## Decision Tree

### Training
```{r}
library(C50)

model_tree <- C5.0(as.factor(fourth_down_converted) ~ ., data = GFI_small)
summary(model_tree)
```

### Predicting
```{r}
predict_tree <- predict(model_tree, GFI_small)

treecm <- confusionMatrix(predict_tree, as.factor(GFI_labels))
treecm
```


## Summary Kappas
Kappa for each model is as follows:

- Logistic Regression: `r glmcm$overall["Kappa"]`
- KNN: `r knncm$overall["Kappa"]`
- ANN: `r anncm$overall["Kappa"]`
- SVM: `r svmcm$overall["Kappa"]`
- Decision Tree: `r treecm$overall["Kappa"]`

# Stacked Model

## Data Frame of all Predictions
```{r}
predictions <- as.data.frame(GFI_labels)
predictions$glm <- predict_glm
predictions$knn <- knn_pred
predictions$ann <- predict_ann
predictions$svm <- predict_svm
predictions$tree <- predict_tree

str(predictions)
summary(predictions)
```

## Split into Train and Validation Sets

```{r}
set.seed(123)
train_set <- sample(1:nrow(predictions), 0.7 * nrow(predictions))

# Training set
train_data <- predictions[train_set, ]
str(train_data)

val_data <- predictions[-train_set, ]
str(val_data)
```


## Decision Tree

```{r}
library(C50)

set.seed(123)

second_level_tree <- C5.0(as.factor(GFI_labels) ~ ., data = train_data)
summary(second_level_tree)
```

## Validate Decision Tree

```{r}
predict_second_level <- predict(second_level_tree, val_data)

plot(second_level_tree)

library(caret)
confusionMatrix(predict_second_level, as.factor(val_data$GFI_labels))
```

# Decision Predictions

(Jim Harbaugh please read this part)

We will now create a function that uses the trained model to determine if a team should "Go For It" on 4th Down.


## Stacked Logistic Model
First, we want a stacked model that outputs a probability, and not a binary response. We will use a logistic stacked model instead of a decision tree. KNN will not be used because it is hard to form a lightweight function.

### Training
```{r, cache=TRUE}
stacked_glm <- glm(GFI_labels ~ . - knn, data = train_data)
summary(stacked_glm)
```

## Decision Function

Now we will define the actual decision function. The function requires as input a data frame with the following fields:

yardline_100, half_seconds_remaining, game_half, goal_to_go, ydstogo, play_type, shotgun, timeout, posteam_score, defteam_score, away_score, home_score, season, home_team, posteam, roof, down, posteam_timeouts_remaining, defteam_timeouts_remaining

It requires a data frame called models that contains the following fields:

ann_model, svm_model, glm_model, tree_model, stacked_glm


```{r}
gfi <- function(df, models, oppteam) {
  model_outs <- data.frame(
    "glm" = predict(models$glm_model, newdata = df),
    "ann" = compute(models$ann_model, df)$net.result,
    "svm" = predict(models$svm_model, df),
    "tree" = predict(models$tree, df)
  )

  library(neuralnet)
  prob_success <- predict(models$stacked_glm, model_outs)

  pos_df <- df
  pos_df$down <- 1
  pos_df$ydstogo <- 10
  pos_df$yardline_100 <- pos_df$yardline_100 - pos_df$ydstogo

  success <- nflfastR::calculate_expected_points(pos_df) %>%
    dplyr::select(ep)

  opp_df <- df
  opp_df$posteam <- oppteam
  opp_df$down <- 1
  opp_df$ydstogo <- 10
  opp_df$yardline_100 <- 100 - pos_df$yardline_100

  failure <- nflfastR::calculate_expected_points(opp_df) %>%
    dplyr::select(ep)

  return(ifelse((prob_success * success$ep + (1 - prob_success) * failure$ep * -1) > 0, "Go For It", "Don't Go For It"))
}
```

Now that the function has been defined we need a scenario:

```{r}
input_1 <- data.frame(
  "fourth_down_converted" = 0,
  "yardline_100" = 10,
  "half_seconds_remaining" = 120,
  "game_half" = 1,
  "goal_to_go" = 0,
  "ydstogo" = 5,
  "play_type" = 1,
  "timeout" = 0,
  "total_home_score" = 21,
  "total_away_score" = 3,
  "posteam_score" = 21,
  "defteam_score" = 3,
  "special_teams_play" = 0,
  "away_score" = 3,
  "home_score" = 21,
  "season" = 2018,
  "home_team" = "DET",
  "posteam" = "DET",
  "roof" = "outdoor",
  "down" = 3,
  "posteam_timeouts_remaining" = 2,
  "defteam_timeouts_remaining" = 2
)
```

We also need the trained models:
```{r}
models <- data.frame(
  "ann_model" = model_ann,
  "svm_model" = model_svm,
  "glm_model" = model_glm,
  "tree_model" = model_tree,
  "stacked_glm" = stacked_glm
)
```

Run the function on the sample input
```{r}
gfi(input_1, models, "CHI")
```
