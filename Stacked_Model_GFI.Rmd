---
title: "Stacked Model (GFI)"
author: "Hip Hip Array"
date: "12/03/2021"
output:
  html_document:
    code_folding: hide
    highlight: tango
    theme: united
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

# Install packages and Load Data

```{r}
library(nflfastR)

future::plan("multisession")
pbp <- load_pbp(2010:2020, file_type = "qs")
str(pbp)

# library(gdata)
# pbp$year <- left(pbp$game_date, n=4)
```

## Set up "Going For It" (GFI) Data
```{r}
# Only pull plays that are a fourth down attempt
GFI <- pbp[!is.na(pbp$play_type), ]
GFI <- GFI[!is.na(GFI$play_id) & GFI$down == 4 & GFI$punt_attempt == 0 & GFI$field_goal_attempt == 0 & GFI$play_type != "no_play" & GFI$play_type != "qb_kneel", ]
```



## Set up GFI data for all models
```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
library(dplyr)

# Include all variables for step function, exclude NA rows
GFI_ALL <- GFI[!is.na(GFI$fourth_down_converted), ]

## Randomize the rows in the data (shuffling the rows)
set.seed(123)
GFI_ALL <- slice(GFI_ALL, sample(1:n()))
```


## Variable Selection with GLM

### Training
```{r}
GFI_GLM_ALL <- select(
  GFI_ALL,
  fourth_down_converted,
  season_type,
  week,
  posteam_type,
  yardline_100,
  quarter_seconds_remaining,
  half_seconds_remaining,
  game_seconds_remaining,
  game_half,
  qtr,
  goal_to_go,
  ydstogo,
  play_type,
  shotgun,
  no_huddle,
  home_timeouts_remaining,
  away_timeouts_remaining,
  timeout,
  posteam_timeouts_remaining,
  defteam_timeouts_remaining,
  posteam_score,
  defteam_score,
  score_differential,
  special_teams_play,
  fixed_drive,
  away_score,
  home_score,
  location,
  div_game,
  roof,
  surface,
  home_opening_kickoff
)
GFI_GLM_ALL <- GFI_GLM_ALL %>% mutate_if(is.character, as.factor)

GFI_GLM_ALL <- GFI_GLM_ALL %>% select(where(~ n_distinct(.) > 1))

train_set <- sample(1:nrow(GFI_GLM_ALL), 0.7 * nrow(GFI_GLM_ALL))

# Training set
gfi_glm_train <- GFI_GLM_ALL[train_set, ]
gfi_glm_validation <- GFI_GLM_ALL[-train_set, ]

library(stats)
model_glm <- glm(fourth_down_converted ~ ., data = gfi_glm_train, family = "binomial", control = list(maxit = 100))
model_glm <- step(model_glm, direction = "backward")
summary(model_glm)
```

### Predicting
```{r}
predict_glm <- predict(model_glm, newdata = gfi_glm_validation, type = "response")
predict_glm_factor <- ifelse(predict_glm < 0.5, 0, 1)
predict_glm_factor <- as.factor(predict_glm_factor)
library(caret)
confusionMatrix(predict_glm_factor, as.factor(gfi_glm_validation$fourth_down_converted))
```

Now we will run future models on just inputs the stepwise GLM found significant.

```{r}
# Include only variables we want to use in prediction
GFI_small <- select(
  GFI_ALL,
  fourth_down_converted,
  yardline_100,
  half_seconds_remaining,
  game_half,
  goal_to_go,
  ydstogo,
  play_type,
  shotgun,
  timeout,
  posteam_score,
  defteam_score,
  away_score,
  home_score
)

GFI_small <- GFI_small %>% mutate_if(is.character, as.factor)

GFI_small <- GFI_small %>% select(where(~ n_distinct(.) > 1))

library(janitor)
GFI_small <- clean_names(GFI_small)

# Model Matrix for ANN
GFI_mm <- as.data.frame(model.matrix(~ . - 1, GFI_small))


# Normalize the data
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything
# Create Labels for KNN
GFI_norm <- as.data.frame(lapply(GFI_mm[-1], normalize))
GFI_labels <- GFI_mm[, 1]
```

## GLM

### Training
```{r}
library(stats)
model_glm <- glm(fourth_down_converted ~ ., data = GFI_small, family = "binomial", control = list(maxit = 100))
summary(model_glm)
```

### Predicting
```{r}
predict_glm <- predict(model_glm, newdata = GFI_small, type = "response")
predict_glm_factor <- ifelse(predict_glm < 0.5, 0, 1)
predict_glm_factor <- as.factor(predict_glm_factor)
library(caret)
glmcm <- confusionMatrix(predict_glm_factor, as.factor(GFI_small$fourth_down_converted))
glmcm
```

## ANN

### Training
```{r}
library(neuralnet)
model_ann <- neuralnet(fourth_down_converted ~ ., data = GFI_mm, stepmax = 1000000, hidden = 3)
plot(model_ann)
```

### Predicting
```{r}
library(neuralnet)
predict_ann <- compute(model_ann, GFI_mm)
predict_ann <- predict_ann$net.result
predict_ann_factor <- ifelse(predict_ann < 0.5, 0, 1)
predict_ann_factor <- as.factor(predict_ann_factor)

anncm <- confusionMatrix(predict_ann_factor, as.factor(GFI_mm$fourth_down_converted))
anncm
```


## KNN

### Tuning the hyperparameter k
```{r}
library(class)

best_kappa <- 0
best_k <- 0

train_set <- sample(1:nrow(GFI_norm), 0.7 * nrow(GFI_norm))

# Training set
gfi_knn_train <- GFI_norm[train_set, ]
gfi_knn_validation <- GFI_norm[-train_set, ]

gfi_knn_train_labels <- GFI_labels[train_set]
gfi_knn_validation_labels <- GFI_labels[-train_set]

for (i in 1:100) {
  knn_pred <- knn(
    train = gfi_knn_train, test = gfi_knn_validation,
    cl = gfi_knn_train_labels, k = i
  )

  kappa <- confusionMatrix(as.factor(knn_pred), as.factor(gfi_knn_validation_labels))$overall["Kappa"]

  if (kappa > best_kappa) {
    best_k <- i
    best_kappa <- kappa
  }
}
```

### Training & Predicting
```{r}
library(class)

knn_pred <- knn(
  train = GFI_norm, test = GFI_norm,
  cl = GFI_labels, k = best_k
)

knncm <- confusionMatrix(as.factor(knn_pred), as.factor(GFI_labels))
knncm
```

## SVM

### Tuning the hyperparemeter sigma

```{r}
library(kernlab)
library(class)

best_kappa <- 0
best_sigma <- 0

train_set <- sample(1:nrow(GFI_small), 0.7 * nrow(GFI_small))

# Training set
gfi_svm_train <- GFI_small[train_set, ]
gfi_svm_validation <- GFI_small[-train_set, ]

for (i in c(2:10 %o% 10^(-5:1))) {
  model_svm <- ksvm(fourth_down_converted ~ ., data = gfi_svm_train, kernel = "rbfdot", kpar = list(sigma = i))
  predict_svm <- predict(model_svm, gfi_svm_validation)
  predict_svm <- ifelse(predict_svm < 0.5, 0, 1)

  kappa <- confusionMatrix(as.factor(predict_svm), as.factor(gfi_svm_validation$fourth_down_converted))$overall["Kappa"]

  if (kappa > best_kappa) {
    best_sigma <- i
    best_kappa <- kappa
  }
}
```

### Training
```{r}
library(kernlab)
model_svm <- ksvm(fourth_down_converted ~ ., data = GFI_small, kernel = "rbfdot", kpar = list(sigma = best_sigma))
```

### Predicting
```{r}
predict_svm <- predict(model_svm, GFI_small)
predict_svm <- ifelse(predict_svm < 0.5, 0, 1)

svmcm <- confusionMatrix(as.factor(predict_svm), as.factor(GFI_labels))
svmcm
```

## Decision Tree

### Training
```{r}
library(C50)

model_tree <- C5.0(as.factor(fourth_down_converted) ~ ., data = GFI_small)
summary(model_tree)
```

### Predicting
```{r}
predict_tree <- predict(model_tree, GFI_small)

treecm <- confusionMatrix(predict_tree, as.factor(GFI_labels))
treecm
```


## Summary Kappas
Kappa for each model is as follows:

- Logistic Regression: `r glmcm$overall["Kappa"]`
- KNN: `r knncm$overall["Kappa"]`
- ANN: `r anncm$overall["Kappa"]`
- SVM: `r svmcm$overall["Kappa"]`
- Decision Tree: `r treecm$overall["Kappa"]`

# Stacked Model

## Data Frame of all Predictions
```{r}
predictions <- as.data.frame(GFI_labels)
predictions$glm <- predict_glm
predictions$knn <- knn_pred
predictions$ann <- predict_ann
predictions$svm <- as.numeric(predict_svm)
predictions$tree <- predict_tree

str(predictions)
summary(predictions)
```

## Split into Train and Validation Sets

```{r}
set.seed(123)
train_set <- sample(1:nrow(predictions), 0.7 * nrow(predictions))

# Training set
train_data <- predictions[train_set, ]
str(train_data)

val_data <- predictions[-train_set, ]
str(val_data)
```


## Decision Tree

```{r}
library(C50)

set.seed(123)

second_level_tree <- C5.0(as.factor(GFI_labels) ~ ., data = train_data)
summary(second_level_tree)
```

## Validate Decision Tree

```{r}
predict_second_level <- predict(second_level_tree, val_data)

plot(second_level_tree)

library(caret)
confusionMatrix(predict_second_level, as.factor(val_data$GFI_labels))
```

# Decision Predictions

(Jim Harbaugh please read this part)

We will now create a function that uses the trained model to determine if a team should "Go For It" on 4th Down.


## Stacked Logistic Model
First, we want a stacked model that outputs a probability, and not a binary response. We will use a logistic stacked model instead of a decision tree. KNN will not be used because it is hard to form a lightweight function.

### Training
```{r, cache=TRUE}
stacked_glm <- glm(GFI_labels ~ . - knn, data = train_data)
stacked_glm <- step(stacked_glm, direction = "backward")
summary(stacked_glm)
```

As it turns out, the ann is also not that useful per the stacked glm, so it can be omitted from the function we create too.

## Decision Function

Now we will define the actual decision function. The function requires as input a data frame with the following fields:

yardline_100, half_seconds_remaining, game_half, goal_to_go, ydstogo, play_type, shotgun, timeout, posteam_score, defteam_score, away_score, home_score, season, home_team, posteam, roof, down, posteam_timeouts_remaining, defteam_timeouts_remaining

It requires the following models:

ann_model, svm_model, glm_model, tree_model, stacked_glm

```{r}
gfi <- function(df, stacked_glm, svm_model, tree, glm_model, oppteam) {
  df$game_half <- factor(df$game_half, levels(GFI_small$game_half))
  df$play_type <- factor(df$play_type, levels(GFI_small$play_type))

  model_outs <- data.frame(
    "glm" = predict(glm_model, newdata = df),
    "svm" = predict(svm_model, df),
    "tree" = predict(tree, df)
  )

  glm_logit <- predict(stacked_glm, model_outs)
  
  logit2prob <- function(logit){
    odds <- exp(logit)
    prob <- odds / (1 + odds)
    return(prob)
  }
  
  prob_success <- logit2prob(glm_logit)

  pos_df <- df
  pos_df$down <- 1
  pos_df$ydstogo <- 10
  pos_df$yardline_100 <- pos_df$yardline_100 - pos_df$ydstogo

  success <- nflfastR::calculate_expected_points(pos_df) %>%
    dplyr::select(ep)

  opp_df <- df
  opp_df$posteam <- oppteam
  opp_df$down <- 1
  opp_df$ydstogo <- 10
  opp_df$yardline_100 <- 100 - pos_df$yardline_100

  failure <- nflfastR::calculate_expected_points(opp_df) %>%
    dplyr::select(ep)

  tab <- matrix(c(ifelse((prob_success * success$ep + (1 - prob_success) * failure$ep * -1) > 0, "Go For It", "Don't Go For It"), prob_success, success, failure), ncol = 4, byrow = TRUE)
  colnames(tab) <- c('Go For It?', 'Probability of Success', 'Exp. Points if Success', 'Exp. Points if Fail')
  return(tab)
}
```

Now that the function has been defined we need a scenario:

```{r}
input_1 <- data.frame(
  "yardline_100" = 10,
  "half_seconds_remaining" = 120,
  "game_half" = "Half1",
  "goal_to_go" = 0,
  "ydstogo" = 5,
  "play_type" = "run",
  "shotgun" = 0,
  "timeout" = 0,
  "posteam_score" = 21,
  "defteam_score" = 3,
  "away_score" = 3,
  "home_score" = 21,
  "season" = 2018,
  "home_team" = "DET",
  "posteam" = "DET",
  "roof" = "dome",
  "down" = 3,
  "posteam_timeouts_remaining" = 2,
  "defteam_timeouts_remaining" = 2
)
```

Run the function on the sample input
```{r}
gfi(input_1, stacked_glm, model_svm, model_tree, model_glm, "CHI")
```
