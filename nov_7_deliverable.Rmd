---
title: "Project 2 Progress Update"
author: "Hip Hip Array"
date: "November 7, 2021"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Data

```{r, cache=TRUE}
library(nflfastR)

future::plan("multisession")
pbp <- load_pbp(2010:2020, file_type = "qs")
str(pbp)
```

# Fourth Down Conversion Success

## Set up "Going For It" (GFI) Data
```{r}
#Only pull plays that are a fourth down attempt
GFI <- pbp[!is.na(pbp$play_type), ]
GFI <- GFI[!is.na(GFI$play_id) & GFI$down == 4 & GFI$punt_attempt == 0 & GFI$field_goal_attempt == 0 & GFI$play_type != "no_play" & GFI$play_type != "qb_kneel",]
```



## Set up GFI data for GLM, KNN, and ANN Models
```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
library(dplyr)
GFI_cond <- select(GFI, fourth_down_converted, posteam_type, yardline_100, half_seconds_remaining, game_half, ydstogo, posteam_score, defteam_score, season_type)

GFI_mm <- as.data.frame(model.matrix(~.-1,GFI_cond))

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
GFI_random <- GFI_mm[sample(nrow(GFI_mm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
GFI_norm <- as.data.frame(lapply(GFI_random, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set_2 <- sample(1:nrow(GFI_norm), 1057) 
test_set_3 <- sample(1:nrow(GFI_cond), 1057) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
GFI_train_2 <- GFI_norm[-test_set_2, -match("fourth_down_converted",names(GFI_norm))]
GFI_test_2 <- GFI_norm[test_set_2, -match("fourth_down_converted",names(GFI_norm))]

#Now the response (aka Labels) 
GFI_train_labels <- GFI_norm[-test_set_2, "fourth_down_converted"]
GFI_test_labels <- GFI_norm[test_set_2, "fourth_down_converted"]

#Ann Training and Test Set
GFI_train_ANN <- GFI_norm[-test_set_2,]
GFI_test_ANN <- GFI_norm[test_set_2,]

#Logistic Training and Test Set
GFI_train_glm <- GFI_cond[-test_set_3,]
GFI_test_glm <- GFI_cond[test_set_3,]

```


## Run GLM Model
```{r}
#GLM Model using a handful of predictors
#NOTE: Want to include interaction between year and team, also want to include weather and how long the current drive has been

glm_mod1 <- glm(fourth_down_converted ~ posteam_type + yardline_100 + half_seconds_remaining + game_half + ydstogo + posteam_score + defteam_score + season_type, data = GFI_train_glm, family = binomial)


summary(glm_mod1)
```

### Create Cross Table of GLM model
```{r}
GFI_test_glm$predict_4th_down_prob <- predict(glm_mod1, newdata = GFI_test_glm, type = "response")
GFI_test_glm$predict_4th_down <- ifelse(GFI_test_glm$predict_4th_down_prob >= 0.5, 1, 0)

GFI_test_glm$predict_4th_down <- factor(GFI_test_glm$predict_4th_down, levels = c(0, 1))

# Evaluate glm model results
library(gmodels)
CrossTable(x = GFI_test_glm$fourth_down_converted, y = GFI_test_glm$predict_4th_down, 
           prop.chisq=FALSE)
```


## Neural Network
```{r, cache=TRUE}
library(neuralnet)

# simple ANN with only 1 hidden layer with 3 neurons
ANN_mod1 <- neuralnet(formula = fourth_down_converted ~ ., data = GFI_train_ANN, stepmax=1000000, hidden = 3)
plot(ANN_mod1)
```

### Neural Network model results
```{r}
library(caret)
# obtain model results
model_results <- neuralnet::compute(ANN_mod1, GFI_test_ANN[-1])
# obtain predicted fourth down values
predicted_fourth_down <- model_results$net.result
# examine the correlation between predicted and actual values
cor(predicted_fourth_down, GFI_test_ANN$fourth_down_converted)
```

### Create Cross Table of ANN Model
```{r}
library(gmodels)
prediction <- ifelse(predicted_fourth_down >= 0.5, 1, 0)
confusionMatrix(as.factor(prediction), as.factor(GFI_test_ANN$fourth_down_converted), positive='1')
CrossTable(x=GFI_test_ANN$fourth_down_converted, y=prediction, prop.chisq=FALSE)
```

## KNN Model
```{r}
library(class)
library(caret)
sqrt(nrow(GFI_train_2))

#Run KNN on train data, create predictions for test data
GFI_test_pred <- knn(train = GFI_train_2, test = GFI_test_2,
                      cl = GFI_train_labels, k=56)

#Evaluate model results
library(gmodels)
CrossTable(x = GFI_test_labels, y = GFI_test_pred, 
           prop.chisq=FALSE)
```
 
# Field Goal Success

## Data Cleaning
```{r}
# Consider only non-blocked field goals
field_goals <- pbp[!is.na(pbp$field_goal_attempt) & pbp$field_goal_attempt == 1,]
field_goals <- field_goals[field_goals$field_goal_result != "blocked",]

field_goals$field_goal_result <- as.factor(field_goals$field_goal_result)
field_goals$weather <- as.factor(field_goals$weather)
field_goals$season_type <- as.factor(field_goals$season_type)
field_goals$temp <- ifelse(is.na(field_goals$temp), 72, field_goals$temp)
field_goals$wind <- ifelse(is.na(field_goals$wind), 0, field_goals$wind)
```


## Run Logistic Regression Model for successful field goal attempts
```{r}
field_goal_glm1 <- glm(success ~ temp + wind  + yardline_100 + game_seconds_remaining + 
                         season_type + score_differential, data = field_goals, family = binomial)

summary(field_goal_glm1)
```



## ANN Model

### Getting Train and Test Samples
```{r}
# Normalize data
field_goals <- select(field_goals, temp, wind, yardline_100, game_seconds_remaining, season_type, score_differential, success)

field_goals_mm <- as.data.frame(model.matrix(~.-1,field_goals))

set.seed(12345)

field_goals_random <- field_goals_mm[sample(nrow(field_goals_mm)), ]
field_goals_norm <- as.data.frame(lapply(field_goals_random, normalize))

# Test and traning data
test_set <- sample(1:nrow(field_goals_norm), 2500) #not sure about what value would be good to use
field_goals_train <- field_goals_norm[-test_set, -match("success", names(field_goals_norm))]
field_goals_test <- field_goals_norm[test_set, -match("success", names(field_goals_norm))]

field_goals_train_labels <- field_goals_norm[-test_set, "success"]
field_goals_test_labels <- field_goals_norm[test_set, "success"]

field_goals_train_ANN <- field_goals_norm[-test_set, ] 
field_goals_test_ANN <- field_goals_norm[test_set, ]
```

### Creating Ann Prediction
```{r, cache=TRUE}
library(neuralnet)
field_goals_model <- neuralnet(formula = success ~ . , data = field_goals_train_ANN, hidden = 3)

model_results1 <- neuralnet::compute(field_goals_model, field_goals_test_ANN[-8]) #TODO FIX
predicted_field_goal_made <- model_results1$net.result

prediction1 <- ifelse(predicted_field_goal_made >= 0.1, 1, 0) #TODO FIX
```


### Ann Confusion Matrix
```{r}
prediction1 <- ifelse(predicted_field_goal_made >= 0.5, 1, 0) #TODO FIX
confusionMatrix(as.factor(prediction1), as.factor(field_goals_test_ANN$success), positive='1')
CrossTable(x=field_goals_test_ANN$success, y=prediction1, prop.chisq=FALSE)
```

## Run Knn Model
```{r}
library(class)
library(caret)

knum <- sqrt(nrow(field_goals_train))

field_goals_test_pred <- knn(train = field_goals_train, test = field_goals_test, cl = field_goals_train_labels, k= knum)

library(gmodels)
CrossTable(x = field_goals_test_labels, y= field_goals_test_pred, prop.chisq=FALSE)

```

# Expected Punt Distance

## Data Cleaning
```{r}
punts <- pbp[!is.na(pbp$play_type), ]
punts$play_type <- as.factor(punts$play_type)
summary(punts$play_type)

# Simplifying assumption: Ignore all punts that end in a score, which represent less than 1% of observations
punts <- punts[punts$play_type == "punt" & punts$series_result == "Punt",]
punts$series_result <- as.factor(punts$series_result)
punts$end_spot <- ifelse(punts$touchback, 75, 100 - punts$yardline_100 + punts$kick_distance - punts$return_yards)

punts_relevant <- punts[,c("season_type", "posteam_type", "yardline_100", "week", "game_seconds_remaining", "game_half", "home_timeouts_remaining", "away_timeouts_remaining", "score_differential", "roof", "surface", "temp", "wind", "end_spot")]
punts_relevant[sapply(punts_relevant, is.character)] <- lapply(punts_relevant[sapply(punts_relevant, is.character)], 
                                       as.factor)
punts_relevant$temp <- ifelse(is.na(punts_relevant$temp), 72, punts_relevant$temp)
punts_relevant$wind <- ifelse(is.na(punts_relevant$wind), 0, punts_relevant$wind)

str(punts_relevant)
summary(punts_relevant)
```

### Linear Regression
```{r}
model1 <- lm(formula=end_spot ~ .-game_seconds_remaining -game_half -home_timeouts_remaining, data=punts_relevant)
summary(model1)
```

# Preliminary Conclusions

Given that we still have about a month to work on the project, we believe our preliminary models show a fair amount of progress. We've summarized our progress in each prediction below.

## Fourth Down Conversion Success

Based on models above, we believe we are on track to achieving our objectives. All of the models are reasonably accurate and provide valuable insight with varying levels of interpretability. Our ANN model is our most accurate however it loses interpretability which is important in the context of this project (especially for football coaches trying to make decisions in the game). I would like to improve the models by continuing to test additional predictors such as weather, current drive length and an interaction betweeen year and team. Additionally, we would like to continue testing thresholds "K" values, and neural network steps in order to continue improving our models. Finally, I would like to attempt a decision tree analysis as this would also give coaches interpretability.

## Field Goal Success

The models built to predict field goal success are quite accurate, but they suffer from a lack of missed field goals in the data set. The data set is very skewed. The models, especially the KNN, could benefit from overweighting missed field goals. The ANN could also benefit from a different, deeper structure.

## Expected Punt Distance

The linear regression model has an adjusted R-squared of 0.54. I believe this is quite accurate already. We have yet to split the data into train and test sets to better evaluate the accuracy of the model. The model could also benefit by considering touchbacks as a separate case.
